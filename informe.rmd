---
title: "Trabajo Práctico VI - Encodeador de música"
subtitle: "Link al repositorio: [https://github.com/IgnacioPardo/TP4_TDVI](https://github.com/IgnacioPardo/TP4_TDVI)"
author:
    - "Zoe Borrone - 21G245" 
    - "Luca Mazzarello - 21L720"
    - "Ignacio Pardo - 21R1160"
date: "`r Sys.Date()`"
lang: "es-ES"
output: pdf_document
toc: true
bibliography: src/bibliography.bib
header-includes:
  - \usepackage{pdfpages}
  - \DeclareUnicodeCharacter{0301}{*************************************}
---
\newpage

## Introducción

Para este trabajo práctico se nos puso como objetivo implementar un encodeador de música con redes neuronales para poder obtener un vector representativo de las canciones en un espacio latente. 

Luego, estos vectores encontrados deben poder desencodearse para volver a reproducir el sonido original. Una vez aplicados el encoding y decoding, realizamos un análisis exploratorio para comprender y encontrar relaciones de estos nuevos vectores, los cuales serán explicados en el desarrollo del informe.

La base de datos usada para el desarrollo del mismo fue GTZAN [@gtzan]. Esta cuenta con 1000 canciones, cada una con una duración de 30 segundos. Las canciones están divididas y clasificadas en 10 géneros distintos: blues, classical, country, disco, hip hop, jazz, metal, pop, reggae, y rock. 

Todos los archivos de audio de los que se habla en este informe, tanto los que se encodearon/desencodearon como la musica generada, se encuentran en la carpeta `wavs/`, los modelos entrenados en la carpeta `src/models/` y los plots en la carpeta `src/plots/`.

## 1) Encodear la canción en un vector latente

Comenzamos utilizando la clase brindada en la consigna llamada MusicDataset para cargar y procesar el conjunto de datos de música en formato WAV. Una vez que realizamos esto, separamos los datos en los conjuntos de validación, testeo y entrenamiento. La división que tomamos en cuenta fue fijar los tamaños de los conjuntos de validación y prueba en 100, mientras que el tamaño del conjunto de entrenamiento se calcula para abarcar el resto de los datos. Luego seteamos los DataLoaders respectivos a cada conjunto con un batch_size de 10 y 20. 

Con los distintos conjuntos ya creados, comenzamos con la creación e implementación del primer modelo usado. Para la visualización de las canciones, utilizamos los respectivos espectrogramas y formas de onda de cada canción. En primera instancia nos quedamos y basamos nuestro análisis en los espectrogramas. Para esto modificamos el MusicDataset para que por cada canción devuelva tambien un espectograma y un espectograma Mel a partir de las funciones de Torchaudio. Los espectogramas resultantes tenian una resolución de `512x256` por lo que comenzamos utilizando un autoencoder del estilo de UNET [@ronneberger_2015_unet], cuyas convoluciones eran 2D pero descartando los `skips` ya que "filtraban" información al decoder. Esta idea fue descartada aunque en un comienzo su arquitectura se mantuvo pero con convoluciones 1D.

El primer modelo que utilizamos fue un autoencoder definido en la clase `autoencoder`. Un problema que tenía este modelo era que al decodear la canción, el tamaño obtenido no era exactamente el mismo que el original, por lo que luego de pasar por el bloque de deconvoluciones, le aplicamos una interpolación al tamaño original.

```
    Input Shape      : 110250      torch.Size([1, 1, 110250])
    Encoded Shape    : 55136       torch.Size([1, 32, 1723])
    Decoded Shape    : 110242      torch.Size([1, 1, 110242])
    Output Shape     : 110250      torch.Size([1, 1, 110250])
    Encoding Scale: 1/ 1.9996009866511897
```
La arquitectura del modelo es la siguiente:

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura \\ Autoencoder}}]{src/plots/autoencoder.gv.pdf}

Este modelo no nos obtuvo buenos resultados, ya que el audio generado salia con un ruido constante de fondo. 

[Audio de Test](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/original_test.wav)
[Audio Reconstruido](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/reconstruction_bad_old.wav)

Inicialmente consideramos podía ser por la interpolación por lo que decidimos cambiar la arquitectura del modelo para que el tamaño de salida del decoder sea el mismo que el de entrada. Para esto, cambiamos el tamaño de los kernels de las convoluciones y deconvoluciones, y agregamos una capa de pooling en el encoder y una de unpooling en el decoder.

El modelo resultante, `autoencoder_alt2`, tiene la siguiente arquitectura:

```
    Input Shape      : 110250      torch.Size([1, 1, 110250])
    Encoded Shape    : 55216       torch.Size([1, 28, 1972])
    Decoded Shape    : 110250      torch.Size([1, 1, 110250])
    Output Shape     : 110250      torch.Size([1, 1, 110250])
    Encoding Scale: 1/ 1.9967038539553752
```

\newpage

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura \\ Autoencoder alt 2}}]{src/plots/autoencoder_alt.gv.pdf}

Este modelo tampoco nos obtuvo buenos resultados. Como no estabamos logrando remover el ruido decidimos comenzar con un modelo desde cero implementado en la clase `autoencoder_alt3`. La idea que tuvimos fue mantener el vector encodeado en un tamaño mayor y bajar la cantidad de features.

```
    Input Shape      : 110250      torch.Size([1, 1, 110250])
    Encoded Shape    : 55125       torch.Size([1, 1, 55125])
    Decoded Shape    : 110250      torch.Size([1, 1, 110250])
    Output Shape     : 110250      torch.Size([1, 1, 110250])
    Encoding Scale: 1/ 2.0
```

La arquitectura de este modelo es la siguiente:

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura \\ Autoencoder alt 3}}]{src/plots/autoencoder_alt3.gv.pdf}

\newpage

Para probar la eficiencia del modelo definimos la función de pérdida a usar, y optamos por “Mean Squared Error”. A su vez usamos optimizadores en el proceso de entrenamiento para ajustar los pesos y los sesgos de la red para minimizar la función de pérdida. Para ellos usamos el optimizador Adam.
En un comienzo cuando entrenamos el primer autoencoder obtuvimos mejores resultados con Adam que con SGD, por lo que continuamos usandolo para los próximos modelos. Para finetunear la taza de aprendizaje y los epochs corrimos un GridSearch sobre el modelo `autoencoder_alt3`, y a su vez sobre los modelos `autoencoder_alt4` y `autoencoder_alt5` que se explicarán más adelante. El espacio de busqueda que utilizamos es el siguiente:

```
    lrs = [0.001, 0.005, 0.01, 0.05, 0.0001, 0.0005]
    nums_epochs = [5, 10, 20, 30]
    models = [
        autoencoder_alt3,
        autoencoder_alt4, 
        autoencoder_alt5
    ]
```

A su vez se corrió con un scheduler con `step=4` y `gamma=0.5`.

```
    Mejor autoencoder_alt3:
        Por Mejor Train Loss: lr=0.01, epochs=30
        Por último Train Loss: lr=0.01, epochs=30
        Por Mejor Validation Loss: lr=0.01, epochs=30
        Por último Validation Loss: lr=0.01, epochs=30
    Mejor autoencoder_alt4:
        Por Mejor Train Loss: lr=0.01, epochs=30
        Por último Train Loss: lr=0.01, epochs=30
        Por Mejor Validation Loss: lr=0.01, epochs=30
        Por último Validation Loss: lr=0.01, epochs=30
    Mejor autoencoder_alt5:
        Por Mejor Train Loss: lr=0.005, epochs=30
        Por último Train Loss: lr=0.005, epochs=30
        Por Mejor Validation Loss: lr=0.005, epochs=30
        Por último Validation Loss: lr=0.005, epochs=30
```

A continuación se muestran las curvas de pérdida obtenidas para cada modelo.

![Curvas de pérdida para `autoencoder_alt3`](src/plots/losses_lr=0.01_bs=10_epochs=30,dev=mps:0_scheduler=StepLR_model=autoencoder_alt3.png)

\newpage

![Curvas de pérdida para `autoencoder_alt4`](src/plots/losses_lr=0.01_bs=10_epochs=30,dev=mps:0_scheduler=StepLR_model=autoencoder_alt4.png)

![Curvas de pérdida para `autoencoder_alt5`](src/plots/losses_lr=0.005_bs=10_epochs=30,dev=mps:0_scheduler=StepLR_model=autoencoder_alt5.png)

\newpage

Después de cada epoch, evalúa el modelo en un conjunto de validación y guarda las pérdidas. También guarda las reconstrucciones de un sample de Test en archivos de Numpy a medida que el modelo se va entrenando para poder visualizar su progresión. Además, utilizamos operaciones como `torch.cuda.empty_cache()`, `torch.mps.empty_cache()`, y `gc.collect()` para gestionar eficientemente la memoria RAM y GPU durante el proceso de entrenamiento.

Con el objetivo de encontrar un tamaño de vector óptimo para el análisis, probamos con diferentes configuraciones de autoencoders, que generan salidas de igual tamaño, pero diferentes salidas del autoencoder y del decoder. Estas variaciones se pueden dar por la definición de la arquitectura del autoencoder, ya que la cantidad y tipo de capas puede influir en las variaciones del tamaño de salida. También se pueden dar variaciones por la elección de la dimensionalidad del espacio latente, por las capas de pooling en el encoder usadas o por la definición de los hiper parámetros como stride y padding.

Los 3 modelos con los que obtuvimos mejores rendimientos no cuentan con funciones de activación en el proceso de encodear y desencodear ya que observamos que agregando las mismas, no mejoraba el rendimiento de los modelos. Para llegar a estos partimos de modelos más complejos, con funciones de activación como “ReLU” y “Tahn” y con Batch Normalization, pero fuimos disminuyendo la complejidad de los mismos, obteniendo ademas del “autoencoer_alt_3” los autoencoders “autoencoder alt 4” y “autoencoderalt_5” que logran menores tamaños de salida del encoder.

Los tamaños durante el proceso de encodear y desencodear de los modelos son los siguientes:

### `autoencoder_alt4`

```
    Input Shape      : 110250      torch.Size([1, 1, 110250])
    Encoded Shape    : 36750       torch.Size([1, 1, 36750])
    Decoded Shape    : 110250      torch.Size([1, 1, 110250])
    Output Shape     : 110250      torch.Size([1, 1, 110250])
    Encoding Scale: 1/ 3.0
```

### `autoencoder_alt5`

```
    Input Shape      : 110250      torch.Size([1, 1, 110250])
    Encoded Shape    : 9189        torch.Size([1, 1, 9189])
    Decoded Shape    : 110250      torch.Size([1, 1, 110250])
    Output Shape     : 110250      torch.Size([1, 1, 110250])
    Encoding Scale: 1/ 11.998041136141039
```

Y sus arquitecturas se encuentran en las siguientes figuras:

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura \\ Autoencoder alt 4}}]{src/plots/autoencoder_alt4.gv.pdf}

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura \\ Autoencoder alt 5}}]{src/plots/autoencoder_alt5.gv.pdf}

## Resultados de los modelos entrenados

A continuación se muestran algunos ejemplos de audios encodeados y decodeados con los modelos `autoencoder_alt3`, `autoencoder_alt4` y `autoencoder_alt5`.

### Autoencoder_alt3 - Encoding a 1/2

![Reconstrución Sample de Train con autoencoder_alt3](src/plots/waveform_reconstruction.png)

[Audio Original](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/original_train.wav) - [Audio Reconstruido](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/reconstruction.wav)

![Reconstrución Samples de Train con autoencoder_alt3 de distintas clases](src/plots/waveform_reconstruction_train.png)

\newpage

Otra cosa que probamos fue hacer un post procesamiento de la señal decodeada para mejorar y para eliminar el ruido que se generaba en el proceso de encodear y decodear. Para esto utilizamos por una parte un proceso de reescalado de la señal, y por otra parte un proceso de filtrado "High-Pass" de la señal usando `Scipy`.

![Postprocesamiento de la señal decodeada con autoencoder_alt3](src/plots/waveform_reconstruction_rescale_hp.png)

Notamos que, aunque minimo, la MSE entre la señal con el postprocesamiento y la original es menor que la MSE respecto de la señal sin postprocesamiento.

| MSE Decoding | MSE Dec + Rescale | MSE Dec + Rescale + HP |
|--------------|-------------------|------------------------|
| 0.002412     | 0.002450          | 0.002352               |

\newpage

### Autoencoder_alt4 - Encoding a 1/3

Para el mismo sample de Train que utilizamos para el modelo `autoencoder_alt3`, obtuvimos los siguientes resultados con el modelo `autoencoder_alt4`.

![Reconstrución Sample de Train con autoencoder_alt4](src/plots/waveform_reconstruction_alt4.png)

[Audio Original](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/original_train.wav) - [Audio Reconstruido](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/reconstruction_alt4.wav)

### Autoencoder_alt5 - Encoding a 1/12

Finalmente, para el mismo sample de Train que utilizamos para los modelos `autoencoder_alt3` y `autoencoder_alt4`, obtuvimos los siguientes resultados con el modelo `autoencoder_alt5`.

![Reconstrución Sample de Train con autoencoder_alt5](src/plots/waveform_reconstruction_alt5.png)

[Audio Original](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/original_train.wav) - [Audio Reconstruido](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/reconstruction_alt5.wav)

### Comparación de MSE entre Autoencoders

El siguiente cuadro muestra la comparación de las MSE obtenidas en un batch con los distintos modelos.

| | Sample | Alt3 | Alt4 | Alt5 |
|---|---|---|---|---|
|-| jazz | 0.0024 | 0.0026 | 0.0036 | 
|-| metal | 0.0014 | 0.0015 | 0.0014 | 
|-| jazz | 0.0018 | 0.0013 | 0.0019 | 
|-| pop | 0.0185 | 0.0222 | 0.0319 | 
|-| pop | 0.0004 | 0.0001 | 0.0001 | 
|-| metal | 0.0164 | 0.0214 | 0.0215 | 
|-| reggae | 0.0010 | 0.0004 | 0.0005 | 
|-| hiphop | 0.0093 | 0.0070 | 0.0068 | 
|-| classical | 0.0004 | 0.0001 | 0.0001 | 
|-| reggae | 0.0062 | 0.0061 | 0.0090 | 
| Media|-| 0.0058 | 0.0063 | 0.0077 | 
| Min |-| 0.0004 | 0.0001 | 0.0001 | 
| Max |-| 0.0185 | 0.0222 | 0.0319 | 

### Sobre datos de Test

A medida que el modelo se va entrenando, implementamos que se guarden las reconstrucciones de un sample de Test en archivos de Numpy para poder visualizar su progresión. El audio de Test que utilizamos es el siguiente:

[Audio de Test](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/original_test.wav)
[Reconstrucción de Test con autoencoder_alt3](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_test_epoch_rec_wav_4)

![Reconstrución Sample de Test con autoencoder_alt3](src/plots/reconstructed_waveform_vs_og_.png)

Al igual que para los datos de Train, para los datos de Test también obtuvimos mejores resultados con el modelo `autoencoder_alt3`. A continuación se muestran las perdidas para distintos samples de Test.

![Reconstrución Sample de Test con autoencoder_alt3](src/plots/waveform_reconstruction_epochs_test.png)

Por último nos interesaba ver la onda de la señal decodeada contra la original para ver si se podía observar alguna diferencia entre las mismas. El siguiente gráfico muestra la onda de la señal original y la decodeada con el modelo `autoencoder_alt3` con zoom para poder observar mejor las diferencias.

![Onda de la señal original y la decodeada con autoencoder_alt3](src/plots/waveform_reconstruction_zoom.png)

Pudimos notar que la onda parece acompañar a la original, pero con un ruido constante de fondo en forma de picos.

\newpage

## 2) Análisis exploratorio de los vectores latentes

### Analisis de Componentes Principales

Para el análisis planteado, comenzamos utilizando el método PCA (Análisis de componentes principales) para reducir la dimensionalidad y poder manipular los encodings en un espacio reducido. Nuestra intención era que estos vectores capturan características importantes o patrones subyacentes en los datos.

Un primer análisis que realizamos fue sobre los vectores latentes obtenidos con el modelo `autoencoder_alt3`. Seleccionamos una canción de muestra de cada clase y obtuvimos su encoding de tamaño `1x55125`. Para aplicar la reducción de dimensionalidad le hicimos un reshape a cada vector para que quede de tamaño `55125 // 5 x 5`. Luego, aplicamos PCA con 2 componentes y graficamos los resultados para observar la dispersión en el espacio 2D, y obtener información sobre la variabilidad y la estructura de las codificaciones en cada clase.


![PCA 2D sobre un sample de cada clase](src/plots/pca_2d_encoding_un_sample.png)

Al hacer PCA a 2 dimensiones no logramos discernir ninguna estructura muy crítica en los datos. A simple vista se puede decir que algunas canciones se encuentran mas dispersas que otras sobre el espacio de las componentes principales. Para continuar la exploración, en vez de encodear una canción de cada clase, encodeamos 10 canciones de cada clase, de esta forma nuestro tensor sería "representativo" de una clase y no de una canción en particular.

![PCA 2D sobre 10 samples de cada clase](src/plots/pca_2d_encoding_10_samples.png)

Para ello a partir de un tensor de tamaño `10x1x55125` concatenamos los encodings de las 10 canciones de cada clase y obtuvimos uno de tamaño `1x551250` al que tambien le hicimos un reshape para que quede de tamaño `551250 // 5 x 5`. Luego, aplicamos PCA con 2 componentes y graficamos los resultados para observar la dispersión en el espacio 2D, y obtener información sobre la variabilidad y la estructura de las codificaciones en cada clase.

\newpage

Los resultados que obtuvimos no lograron ser muy diferentes a los anteriores, por lo que decidimos hacer PCA a 3 dimensiones para poder visualizar mejor los datos. En algunos batches de 10 canciones de cada clase, algunas clases parecían seguir seguir un patron respecto a las componentes principales, pero en otros batches no se observaba nada, por lo que decidimos continuar la exploración por otro lado. A continuación se muestran los resultados obtenidos:

![PCA 3D sobre un sample de cada clase](src/plots/pca_3d_encoding_un_sample.png)
![PCA 3D sobre 10 samples de cada clase](src/plots/pca_3d_encoding_10_samples.png)


\newpage

### Similitud entre canciones

Nuevamente, partimos de los vectores originales, tanto los codificados como los decodificados, y tomamos 10 muestras de cada clase. Calculamos la similitud coseno entre las muestras de cada clase, considerando tanto los vectores originales de una canción en una clase como los vectores originales de otra canción de la misma clase, así como los vectores codificados de una clase y los vectores codificados de una canción de la misma clase. Este análisis nos permitió observar las similitudes o diferencias entre las muestras.

![Similitud Coseno entre canciones de la misma clase](src/plots/similitud_coseno_og_enc_dec_intra_clase.png)

A partir de los resultados concluimos que, entre canciones de las mismas clases, tanto los vectores originales como decodificados son ortogonales entre si (es decir no son iguales), pero los vectores codificados resultan ser muy similares, esto lo podemos ver ya que la media de las distribuciones de similitud coseno entre canciones de la misma clase ronda los $\tilde 0.8$, nuestra primera hipótesis es que esto se debe a que las canciones de una misma clase tienen características similares, por lo que al encodearlas se obtienen vectores similares entre si.

\newpage

### Similitud entre clases

Por último, realizamos un análisis de similitud entre clases. Para ello, con las mismas 10 muestras de cada clase del item anterior calculamos la similitud coseno entre los encodings de todos los samples. No consideramos los vectores originales ya que no nos interesaba la similitud entre canciones ya que si las canciones entre la misma clase daban ortogonales entre si, asumimos que entre clases iba a ser lo mismo. Este análisis nos permitió observar las similitudes o diferencias entre las clases en nuestros encodings.

![Similitud Coseno entre canciones de distintas clases](src/plots/similitud_coseno_enc_inter_clase.png)

Los resultados refutaron nuestra hipotesis ya que las distribuciones de similitud coseno entre canciones de distintas clases resultaron ser muy similares a las distribuciones de similitud coseno entre canciones de la misma clase. Esto nos dió un mejor entendimiento del porque los resultados de PCA no fueron los esperados, ya que al encodear las canciones, se pierde mucha información y se obtienen vectores muy similares entre si.

\newpage

## 3) Encodear música nueva

Una vez entrenado el modelo y pasado nuestro propio test de validación auditivo, es decir, una vez que al escuchar la reconstrucción de test estabamos conformes con el resultado, utilizamos nuevas canciones fuera del dataset para corroborar que funcione y el resultado fue positivo. El resultado fue similar al que obtuvimos con las canciones del dataset. Para esta sección no usamos todos los modelos que en entrenamiento, sino que usamos solo el `autoencoder alt 3`, previamente explicado, porque como solo reduce la entrada a la mitad, permite obtener mejores resultados para estas canciones nuevas al dataset. Los audios que elegimos para esta sección son los siguientes:

### Canciones fuera del Dataset de Otros Generos

Por una parte probamos con canciones de otros géneros que no se encuentran en el dataset.

#### Cumbia Argentina

- [No me arrepiento de este amor - Gilda](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/gilda.wav)
- [Nunca Me Faltes - Antonio Rios](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/nunca_me_faltes.wav)

#### Rap y Trap

- [Rap God - Eminem](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/eminem.wav)
- [Givenchy - Duki](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/duki.wav)

#### Funk

- [J.A.M. - Cory Wong](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/jam.wav)

#### Rock Progresivo

- [All Falls Apart - Polyphia](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/polyphia.wav)

### Audios Varios
Ademas probamos con otros audios que no son canciones, sino que son sonidos o audios de otros tipos.

- [Ringtone Nokia](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/nokia.wav)
- [Himno Argentino Cancha](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/himno.wav)
- [Trompetas de la 12 - Boca](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/boca.wav)
- [Audio Solo de Guitarra](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/solo.wav)
- [Sorting Sounds - Stable Sort GCC](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/stable_sort_gcc.wav)
- [Relato Siamofuori della Copa - Italia 90](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/data/new_songs/siamofuori.wav)

Para poder utilizarlas, las adaptamos al mismo formato que las canciones del dataset. Todas los audios fueron descargados de YouTube como archivos `MP3` y luego convertidos a formato WAV. Al leerlos con `torchaudio` nos dimos cuenta que tenían distintos canales, por lo que los convertimos a mono. Luego, los recortamos a 5 segundos y los re-sampleamos a 22050 Hz ya que se habían grabado en 44100 Hz.

### Resultados

A oído los resultados fueron satisfactorios, ya que se puede escuchar la canción original en la reconstrucción. A continuación se muestran algunos ejemplos de audios encodeados y decodeados con el modelo `autoencoder_alt3`. Los audios parecen provenir de adentro de un contender pero no se escucha ruido de fondo como esperabamos.

![Reconstrución Canción de Cumbia Argentina con autoencoder_alt3](src/plots/waveform_reconstruction_new_songs.png)

En cuanto a la perdida, el MSE en la mayoría de los casos es un orden de maginitud mayor que el obtenido con las canciones del dataset, pero sigue siendo al rededor de $0.01$ por lo que consideramos que es un buen resultado.

| Canción | MSE | WAV |
|---------|-----|-----|
| gilda | MSE: 0.00939| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_gilda) |
| nokia | MSE: 0.02169| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_nokia) |
| polyphia | MSE: 0.01835| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_polyphia) |
| himno | MSE: 0.01384| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_himno) |
| solo | MSE: 0.00458| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_solo) |
| boca | MSE: 0.00275| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_boca) |
| stable_sort_gcc | MSE: 0.01060| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_stable_sort_gcc) |
| nunca_me_faltes | MSE: 0.00844| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_nunca_me_faltes) |
| jam | MSE: 0.00963| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_jam) |
| duki | MSE: 0.01171| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_duki) |
| eminem | MSE: 0.01434| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_eminem) |
| siamofuori | MSE: 0.01181| [Audio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/rec_new_siamofuori) |

\newpage

## 4) Generación de Música

### A partir de tensores nuevos

Con la intención de crear música original, comenzamos utilizando un tensor con valores aleatorios y procedimos a decodificarlo. Sin embargo, el resultado de este proceso fue esencialmente ruido. 

![Reconstrución de un tensor aleatorio con autoencoder_alt3](src/plots/waveform_artificial_ruido.png)

[Audio a partir de un tensor aleatorio](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/artificial_ruido.wav)

Posteriormente, exploramos la opción de un vector cuyos valores estaban determinados por la función seno.

![Reconstrución de un tensor con valores seno con autoencoder_alt3](src/plots/waveform_artificial_seno.png)
[Audio a partir de un tensor generado por la función seno](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/artificial_seno.wav)

Algo que nos sorpendio es que la onda generada no se encuentra centrada en 0 por lo que no se puede escuchar. Esto es porque nuestro modelo no cuenta con una función de activación estilo "Tanh" que nos permita obtener valores entre -1 y 1. Para poder escuchar la onda le sumamos la media de la onda pero igual no se logra escuchar mucho.

### A partir de encodings de canciones

Después de estas primeras experiencias, decidimos tomar dos canciones que, por una parte sean pertenecientes a la misma categoría, y ademas, a partir de una implementación de detección de beats por minuto (BPM) [@python_bpm], nos aseguramos que las canciones sean mas cercanas aún, para luego sumar sus codificaciones. Al decodificar este resultado, notamos que el sonido resultante era similar a la suma de las dos canciones originales.

![Reconstrución de la suma de dos canciones con autoencoder_alt3](src/plots/suma_encodings.png)

[Sample 1](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/to_sum_sample_1.wav) | [Sample 2](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/to_sum_sample_2.wav) | [Deocde de Suma de Encodings](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/sum_encodings.wav)

\newpage

Por último, decidimos tomar otro par de canciones de las mismas propiedades para luego promediar sus codificaciones. En este caso obtuvimos un muy buen resultado, cabe resaltar que en parte fue el azar pero también se debe a que las canciones elegidas son muy parecidas entre si por haber hecho la selección de BPMs cercanos.

![Reconstrución del promedio de dos canciones con autoencoder_alt3](src/plots/avg_encodings.png)

[Sample 1](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/to_avg_sample_1.wav) | [Sample 2](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/to_avg_sample_2.wav) | [Deocode de Promedio Encodings](https://github.com/IgnacioPardo/TP4_TDVI/raw/main/src/wavs/avg_2encs.wav.wav)

\newpage

### Importancia de Atributos

Por último, entrenamos un clasificador Random Forest a partir de los encodings de las canciones del dataset. Para cada encoding, toma los $n$ atributos más importantes y los descartamos con el objetivo de observar cómo estos afectan en la reconstrucción. Decidimos primero anular los primeros 1000 atributos mas significativos a la clasificación y obtuvimos los siguientes resultados:

![Reconstrución de un encoding con los primeros 1000 atributos anulados con autoencoder_alt3](src/plots/encoding_mods.png)

![Reconstrución de un encoding con los primeros 1000 atributos anulados con autoencoder_alt3 - Zoom](src/plots/encoding_mods_zoom.png)

- MSE - Sample Original y Reconstruido: 0.0049
- MSE - Sample Original y Reconstruido con los primeros 1000 atributos anulados: 0.0084
- MSE - Reconstruido y Reconstruido con los primeros 1000 atributos anulados: 0.0034

Para explorar más a fondo, ploteamos la MSE entre el sample original y el reconstruido a partir de encodings con los primeros $n$ atributos anulados, para distintos valores de $n$.

![MSE entre el sample original y el reconstruido a partir de encodings con los primeros $n$ atributos anulados](src/plots/mse_vs_n_mods.png)

\newpage

## Otras ideas descartadas

### Espectogramas Mel

En un comienzo utilizamos Espectogramas Mel [@melspectrogram] para entrenar los modelos. La arquitectura del modelo toma un tamaño de entrada de `512x256` (131072 features) y lo reduce a `128x32x16]` (4096 features en 16 canales`. 

El siguiente gráfico muestra un esfuerzo de entrenamiento con este tipo de espectogramas. Arriba a la izquierda el espectograma Mel original, de izquierda a derecha y de arriba a abajo, el espectograma Mel reconstruido luego de cada epoch.

![Reconstrucción de un espectograma Mel durante entrenamiento](src/plots/recs_specs.png)

![Onda de la señal original y la decodeada](src/plots/rec_spec_2_wave.png)

Este modelo se descartó luego de conversar con la catedra ya que no es lo que se esperaba del trabajo práctico. Sin embargo nos pareció interesante mostrar los resultados obtenidos. Ademas, el modelo que utilizamos para entrenar los espectogramas Mel dió origen al modelo `autoencoder` que detallamos al comienzo del informe.

\includepdf[pages=1, pagecommand={\thispagestyle{plain}\subsection{Arquitectura UNET}}]{src/plots/unet.gv.pdf}

### Función de perdida

A pesar de que la función de pérdida de la que hablamos durante el informe es MSE, en un instante del desarrollo planteamos una nueva función de perdida. Para ello entrenamos un modelo XGBoost sobre los datos de Train para clasificar a partir de una onda el genero al que pertenece, esto lo hicimos para ademas de tener en cuenta el error cuadrático medio, tener en cuenta el error de clasificación de la reconstrucción.

La función de perdida se define como:

$$
    L = \alpha * MSE + (1 - \alpha) * CE
$$

Donde $MSE$ es el error cuadrático medio entre la onda original y la reconstruida, $CE$ es el error de clasificación de la reconstrucción y $\alpha$ es un hiper parámetro que pondera la importancia de cada error. Para este caso, $\alpha = 0.5$.

Esta función de perdida no nos dió buenos resultados, ya que el clasificador XGBoost estaba bastante sobreajustado a las canciones de Train y penalizaba mucho la reconstrucción de canciones de Test. Ademas nos pareció que el computo extra que se necesitaba para calcular el error de clasificación no valía la pena por lo que decidimos descartar esta función de perdida.

### Data augmentation de las canciones

Otra idea que tuvimos fue hacer data augmentation de las canciones. Planteamos dos metodos de data augmentation, uno que consistía en agregar ruido blanco a la señal y otro que consistía en hacer un pitch shift de la señal. Para agregar ruido blanco a la señal, utilizamos la función `torch.randn_like` para generar un tensor de ruido blanco del mismo tamaño que la señal y luego lo sumamos a la señal. Para hacer un pitch shift de la señal utilizamos la transformación `PitchShift` de `Torchaudio` [@hira_2023_audio].

Ambas implementaciones de data augmentation se encuentran en el notebook de la entrega. Se evaluaron sobre el primer autoencoder y no haber dado mejores resultados se comento esa sección y no se volvió a utilizar sobre los futuros modelos aunque podría haber sido interesante revisitarlo.

\newpage

## Referencias

<div id="refs"></div>
